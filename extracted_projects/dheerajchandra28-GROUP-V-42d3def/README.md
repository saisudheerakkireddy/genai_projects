# Advanced Offline RAG Assistant: The Uni-RAG Agent

![Python](https://img.shields.io/badge/Python-3.11+-blue?style=for-the-badge&logo=python)
![React](https://img.shields.io/badge/React-18-blue?style=for-the-badge&logo=react)
![FastAPI](https://img.shields.io/badge/FastAPI-0.100%2B-green?style=for-the-badge&logo=fastapi)
![Tailwind CSS](https://img.shields.io/badge/Tailwind_CSS-3-38B2AC?style=for-the-badge&logo=tailwind-css)
![LangChain](https://img.shields.io/badge/LangChain-0.2%2B-green?style=for-the-badge)
![Tesseract](https://img.shields.io/badge/OCR-Tesseract-orange?style=for-the-badge)
![CUDA](https://img.shields.io/badge/NVIDIA%20CUDA-Enabled-76B900?style=for-the-badge&logo=nvidia)

An advanced, **Zero-Trust**, and completely **offline Retrieval-Augmented Generation (RAG)** system built with a **React frontend**, **FastAPI backend**, and **LangChain**. This application enables secure, local conversational AI capable of processing various document types (including scanned PDFs via OCR) and providing answers grounded in the provided context.

---

## üìñ Table of Contents
- [About The Project](#-about-the-project)
- [‚ú® Key Features](#-key-features)
- [üèõÔ∏è System Architecture](#Ô∏è-system-architecture)
- [üõ†Ô∏è Tech Stack](#-tech-stack)
- [üöÄ Getting Started](#-getting-started)
- [üíª Usage & Demo](#-usage--demo)
- [üõ£Ô∏è Future Work](#-future-work)

---

## üèõÔ∏è About The Project

The **Uni-RAG Agent** addresses the need for powerful conversational AI in environments demanding **Zero-Trust Data Privacy**. Unlike cloud-dependent solutions, this system runs entirely locally using local Large Language Models (LLMs), local vector storage, and local OCR processing, accelerated by GPU where available. It features a modern web interface built with React for user interaction.

While the original concept involved a more complex agentic workflow (hence the name), the current implementation focuses on a robust, offline RAG pipeline with enhanced document processing capabilities.

---

## ‚ú® Key Features

* **Advanced RAG Pipeline:** Efficiently retrieves relevant information from indexed documents to provide context-aware answers.
* **Zero-Trust Data Privacy (100% Offline):** After initial model downloads, the entire application (frontend, backend, LLM) runs without internet access, ensuring data never leaves the local machine.
* **OCR for Scanned PDFs & Handwriting:** Integrates **Tesseract OCR** via `pdf2image` and `pytesseract` to extract text from image-based PDFs, enabling processing of scanned documents and those containing basic handwriting.
* **Grounded Answers:** Provides source document and page number information (when available via OCR/metadata) alongside answers generated by the local LLM (**Ollama/phi3**), promoting transparency.
* **GPU Acceleration (CUDA):** Leverages an NVIDIA GPU for faster document embedding (`HuggingFaceEmbeddings`) and offline **Text-to-Speech (TTS)** generation (`Coqui TTS`).
* **Persistent Knowledge Base:** Indexes and searches a local folder of documents (`.pdf`, `.docx`), automatically processing new files added to the designated directory.
* **Single File Chat with Caching:** Allows temporary chat sessions with individually uploaded files, using in-memory caching (based on file hash) to avoid reprocessing the same file repeatedly.
* **Modern Web Interface:** A responsive frontend built with **React**, **Vite**, **Tailwind CSS**, and **shadcn/ui** for a clean user experience. Includes basic 3D elements via **React Three Fiber**.

---

## üèõÔ∏è System Architecture

The system follows a client-server architecture designed for local operation:

1.  **User Input (React Frontend):** The user interacts via the web interface (running on e.g., `localhost:5173`), uploading files or typing queries.
2.  **API Call (FastAPI Backend):** The React frontend sends requests (file uploads, queries) to the FastAPI backend API (running on `localhost:8000`).
3.  **Backend Processing:**
    * **File Upload (`/chat-file`):**
        * Calculates file hash, checks in-memory cache.
        * *Cache Miss:* Saves temp file, uses OCR (PDF) or Docx2txtLoader (DOCX) to extract text, splits text, generates embeddings (on GPU), creates temporary Chroma vector store, stores in cache.
        * *Cache Hit:* Retrieves existing vector store from cache.
    * **Folder Chat (`/chat-folder`):** Uses the persistent Chroma vector store loaded at startup.
    * **Knowledge Base Update (`/refresh-knowledge-base`, or on startup):** Scans the `KNOWLEDGE_BASE_PATH`, processes new files using OCR/Docx2txtLoader, splits, embeds, and adds them to the persistent Chroma store.
4.  **Retrieval:** The backend uses the appropriate vector store (temporary or persistent) to find text chunks relevant to the user's query.
5.  **Generation:** Relevant context and the original query are formatted into a prompt and sent to the local LLM (`phi3` via Ollama on `localhost:11434`).
6.  **Response Synthesis:** The LLM generates an answer based *only* on the provided context.
7.  **TTS (Optional):** The generated text response is converted to speech using the local Coqui TTS model (on GPU).
8.  **API Response (FastAPI -> React):** The backend sends the text answer, source information, and the URL for the generated audio file back to the React frontend.
9.  **Display (React Frontend):** The frontend displays the chat messages, source info, and provides an audio player.


<img width="500px" height="500px" alt="Arcitecture Diagram" src="https://github.com/user-attachments/assets/7b432ad4-d4a8-4ea0-a046-ee1479259592" />



---

## üõ†Ô∏è Tech Stack

### üíª Frontend

| Category             | Tools Used                                     |
| :------------------- | :--------------------------------------------- |
| **Framework** | React (v18+, TypeScript)                       |
| **UI Library** | shadcn/ui                                      |
| **Styling** | Tailwind CSS                                   |
| **Animation** | Framer Motion                                  |
| **3D Graphics** | React Three Fiber (R3F), Drei                  |
| **State Management** | React Context API / useState & useEffect hooks |
| **API Client** | Axios                                          |
| **Build Tool** | Vite                                           |

### üîß Backend

| Category          | Tools Used             |
| :---------------- | :--------------------- |
| **Language** | Python (3.11+)         |
| **API Framework** | FastAPI                |
| **Server** | Uvicorn                |
| **Dependencies** | `requirements.txt`     |

### üß† AI / Processing

| Category                | Tools Used                                                       | Notes                     |
| :---------------------- | :--------------------------------------------------------------- | :------------------------ |
| **AI Orchestration** | LangChain (Loaders, Splitters, Embeddings, Prompts, VectorStores) | Direct RAG Chain          |
| **Local LLM** | Ollama (serving `phi3` model)                                    | Runs on `localhost:11434` |
| **Vector Database** | ChromaDB                                                         | Stores locally            |
| **Embeddings Model** | Sentence-Transformers (`all-MiniLM-L6-v2`)                       | Runs on CUDA via PyTorch  |
| **TTS Engine** | Coqui TTS                                                        | Runs on CUDA via PyTorch  |
| **OCR Engine** | Tesseract OCR                                                    | System installation       |
| **PDF -> Image (OCR)** | pdf2image / Poppler Utilities                                    | System installation       |
| **DOCX Loader** | Docx2txtLoader (`langchain_community`)                           | Python library            |

---

## üöÄ Getting Started

### Prerequisites

* **Python 3.11+**
* **Node.js & npm** (or yarn/bun) for the frontend.
* An **NVIDIA GPU** with CUDA installed (Recommended for GPU acceleration of embeddings/TTS, CPU fallback possible but slower). Check PyTorch compatibility.
* **Ollama** installed and running (`ollama serve`). [Link to Ollama](https://ollama.com/)
* **Tesseract OCR Engine** installed AND **added to your system PATH**. [Link to Tesseract Installers](https://github.com/UB-Mannheim/tesseract/wiki)
* **Poppler Utilities** installed AND **added to your system PATH**. [Link to Windows Poppler Binaries](https://github.com/oschwartz10612/poppler-windows/releases) (or use package manager like `brew` or `apt`).

### Installation Guide

Perform these steps once with an internet connection to download models. Afterward, the system can run offline.

**1. Setup Backend (Python Environment & Dependencies)**

* Clone this repository.
* Navigate to the backend directory (where `app.py` resides).
* Create and activate a Python virtual environment:
    ```bash
    python -m venv venv_py11
    # Windows:
    .\venv_py11\Scripts\activate
    # Linux/macOS:
    # source venv_py11/bin/activate
    ```
* **Install PyTorch for your CUDA version** (check compatibility at [PyTorch Get Started](https://pytorch.org/get-started/locally/)):
    ```bash
    # Example for CUDA 12.1:
    pip install torch torchvision torchaudio --index-url [https://download.pytorch.org/whl/cu121](https://download.pytorch.org/whl/cu121)
    # Example for CPU only (if no compatible GPU):
    # pip install torch torchvision torchaudio
    ```
* **Install Python Requirements:**
    ```bash
    pip install -r requirements.txt
    pip install pytesseract pdf2image Pillow # For OCR
    # Note: langchain-agents is NOT needed for this version
    ```

**2. Setup Frontend (React)**

* Navigate to your separate frontend project directory (e.g., `rag-frontend`).
* Install Node.js dependencies:
    ```bash
    npm install
    # or yarn install / bun install
    ```

**3. Setup Local LLM & Download AI Models**

* Pull the required LLM via Ollama (if not already done):
    ```bash
    ollama pull phi3
    ```
* **Download Embeddings & TTS Models:** Run the backend server **once** while connected to the internet. This allows LangChain and Coqui TTS to download and cache the necessary models. Start it with:
    ```bash
    # Make sure backend venv is active
    python -m uvicorn app:app --reload
    ```
    Wait until you see messages confirming the embedding model and TTS model have loaded (or failed to load). You can then stop the server (`Ctrl+C`). Subsequent runs can be offline. *Alternatively, modify/create a `setup.py` script to explicitly download/cache these.*

**4. Configuration**

* **Knowledge Base:** Place your `.pdf` and `.docx` files into the folder specified by `KNOWLEDGE_BASE_PATH` in `app.py`.
* **Tesseract Path:** If Tesseract is *not* in your system PATH, you **must** uncomment and set the correct path for `pytesseract.pytesseract.tesseract_cmd` near the top of `app.py`.
* **Poppler Path:** If Poppler is *not* in your system PATH, ensure the `poppler_path` argument in the `convert_from_path` function call within `extract_text_with_ocr` in `app.py` points correctly to your Poppler `bin` directory.

---

## üíª Usage & Demo

1.  **Start Ollama:** Ensure the Ollama service is running. In a terminal:
    ```sh
    ollama serve
    ```
    (Leave this running).
2.  **Start Backend:** Open a **new terminal**, navigate to the backend directory, activate the venv, and run:
    ```bash
    python -m uvicorn app:app --reload
    ```
    Keep this terminal open to monitor logs.
3.  **Start Frontend:** Open **another new terminal**, navigate to the frontend directory, and run:
    ```bash
    npm run dev
    # or yarn dev / bun dev
    ```
4.  **Access App:** Open your browser to the URL provided by Vite (e.g., `http://localhost:5173`).

**Demo:**

* **Folder Chat:** Select "Chat with Knowledge Base". Ask questions about the documents placed in your `KNOWLEDGE_BASE_PATH`. Check the backend logs to see files being processed if they are new.
* **Single File Chat:** Select "Upload Document". Upload a `.pdf` (including scanned ones) or `.docx`. Ask a question. Upload the *same* file again and ask another question ‚Äì observe the backend logs showing a "Cache hit" for faster processing.
* **Verify Sources:** Note the source file/page number displayed with AI answers (accuracy depends on OCR quality for PDFs).
* **Audio Output:** Click the play button on AI messages to hear the TTS output (if the TTS model loaded successfully).

---

## üõ£Ô∏è Future Work

* **Implement Agentic Workflow (ReAct):** Upgrade the backend to use LangChain Agents (`AgentExecutor`, `Tool`, `create_react_agent`) to handle more complex, multi-step queries as originally envisioned.
* **Multimodal RAG:** Integrate Vision-Language Models (e.g., `llava`, `bakllava` via Ollama) for image understanding/Q&A and add speech-to-text (e.g., **Whisper**) for audio input/processing.
* **Improve Caching:** Implement more robust/persistent caching for temporary vector stores instead of just in-memory.
* **UI Enhancements:** Add chat history, theme persistence, and refine UI interactions.
* **Error Handling:** Improve feedback to the user in the frontend when backend errors occur (e.g., OCR failures, model connection issues).
