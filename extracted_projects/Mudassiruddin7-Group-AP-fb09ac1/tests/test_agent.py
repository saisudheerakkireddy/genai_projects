"""
Evaluation Framework for F2 Portfolio Recommender Agent
Tests agentic metrics: Tool Correctness, Argument Accuracy, Output Quality
Demonstrates reproducible, deterministic testing for hackathon judges
"""
import json
import pytest
from typing import Dict, List
import sys
import os

# Add parent directory to path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from portfolio_optimizer import portfolio_optimizer_tool, PortfolioOptimizer
from guardrails import check_input_safety, apply_output_guardrails, GuardrailSystem
from config import TEST_CASES


class AgenticEvaluationMetrics:
    """
    Custom evaluation metrics for agentic AI systems
    Beyond traditional accuracy - measures reasoning quality
    """
    
    @staticmethod
    def tool_selection_accuracy(expected_tool: str, actual_tool: str) -> float:
        """
        Did the agent select the correct tool for the task?
        
        Returns:
            1.0 if correct, 0.0 if incorrect
        """
        return 1.0 if expected_tool == actual_tool else 0.0
    
    @staticmethod
    def argument_correctness(
        expected_args: Dict,
        actual_args: Dict,
        tolerance: float = 0.1
    ) -> float:
        """
        Are the tool arguments correct and appropriately extracted?
        
        Args:
            expected_args: Ground truth arguments
            actual_args: Agent-generated arguments
            tolerance: Acceptable deviation for numeric values
            
        Returns:
            Score 0.0-1.0 based on argument match quality
        """
        if not expected_args:
            return 1.0
        
        score = 0.0
        total_args = len(expected_args)
        
        for key, expected_val in expected_args.items():
            if key not in actual_args:
                continue
            
            actual_val = actual_args[key]
            
            # String comparison
            if isinstance(expected_val, str):
                if expected_val.lower() == actual_val.lower():
                    score += 1.0
            
            # Numeric comparison with tolerance
            elif isinstance(expected_val, (int, float)):
                if abs(expected_val - actual_val) <= (abs(expected_val) * tolerance):
                    score += 1.0
            
            # Exact match for other types
            elif expected_val == actual_val:
                score += 1.0
        
        return score / total_args if total_args > 0 else 0.0
    
    @staticmethod
    def output_format_validity(output: Dict) -> float:
        """
        Is the output correctly formatted and complete?
        
        Returns:
            Score 0.0-1.0 based on format compliance
        """
        required_fields = ['portfolio_weights', 'metrics', 'explanation']
        score = sum([1.0 for field in required_fields if field in output]) / len(required_fields)
        
        # Additional checks
        if 'portfolio_weights' in output:
            weights = output['portfolio_weights']
            if isinstance(weights, dict) and len(weights) > 0:
                total_weight = sum(weights.values())
                # Check if weights sum to ~1.0
                if 0.95 <= total_weight <= 1.05:
                    score += 0.2
        
        return min(score, 1.0)
    
    @staticmethod
    def explanation_quality(explanation: str) -> float:
        """
        Does the explanation contain key required elements?
        
        Returns:
            Score 0.0-1.0 based on explanation completeness
        """
        if not explanation or len(explanation) < 50:
            return 0.0
        
        score = 0.0
        
        # Check for key components
        quality_indicators = [
            'risk',
            'return',
            'portfolio',
            'diversif',
            'volatility',
            'disclaimer',
            'years'
        ]
        
        explanation_lower = explanation.lower()
        matches = sum([1 for indicator in quality_indicators if indicator in explanation_lower])
        
        score = matches / len(quality_indicators)
        
        # Bonus for good length
        if 200 <= len(explanation) <= 1000:
            score += 0.2
        
        return min(score, 1.0)
    
    @staticmethod
    def guardrail_compliance(response: Dict) -> float:
        """
        Were safety guardrails properly applied?
        
        Returns:
            Score 0.0-1.0 based on guardrail compliance
        """
        score = 0.0
        
        # Check for disclaimer
        explanation = response.get('explanation', '')
        if 'disclaimer' in explanation.lower() or 'not financial advice' in explanation.lower():
            score += 0.5
        
        # Check for guardrails metadata
        if 'guardrails_applied' in response:
            score += 0.25
        
        # Check output validation
        if response.get('success', False):
            score += 0.25
        
        return min(score, 1.0)


# ========== Unit Tests ==========

class TestPortfolioOptimizer:
    """Test suite for portfolio optimization tool"""
    
    def test_valid_optimization_low_risk(self):
        """Test: Low risk profile generates conservative portfolio"""
        result = portfolio_optimizer_tool(
            risk_profile='low',
            horizon_years=10
        )
        
        assert 'error' not in result or not result['error']
        assert 'portfolio_weights' in result
        assert 'metrics' in result
        assert result['metrics']['risk_profile'] == 'low'
    
    def test_valid_optimization_high_risk(self):
        """Test: High risk profile generates aggressive portfolio"""
        result = portfolio_optimizer_tool(
            risk_profile='high',
            horizon_years=3
        )
        
        assert 'error' not in result or not result['error']
        assert 'portfolio_weights' in result
        assert result['metrics']['risk_profile'] == 'high'
    
    def test_invalid_risk_profile(self):
        """Test: Invalid risk profile is handled gracefully"""
        result = portfolio_optimizer_tool(
            risk_profile='invalid',
            horizon_years=5
        )
        
        assert 'error' in result
    
    def test_weights_sum_to_one(self):
        """Test: Portfolio weights sum to approximately 1.0"""
        result = portfolio_optimizer_tool(
            risk_profile='medium',
            horizon_years=5
        )
        
        if 'portfolio_weights' in result and result['portfolio_weights']:
            total_weight = sum(result['portfolio_weights'].values())
            assert 0.95 <= total_weight <= 1.05, f"Weights sum to {total_weight}, expected ~1.0"
    
    def test_explanation_generated(self):
        """Test: Human-readable explanation is provided"""
        result = portfolio_optimizer_tool(
            risk_profile='medium',
            horizon_years=7
        )
        
        assert 'explanation' in result
        assert len(result['explanation']) > 100


class TestGuardrails:
    """Test suite for safety guardrails"""
    
    def test_pii_detection_pan(self):
        """Test: PAN card number is detected"""
        is_safe, error = check_input_safety("My PAN is ABCDE1234F")
        assert not is_safe
        assert error is not None
    
    def test_pii_detection_aadhaar(self):
        """Test: Aadhaar number is detected"""
        is_safe, error = check_input_safety("My Aadhaar is 123456789012")
        assert not is_safe
    
    def test_safe_input(self):
        """Test: Normal query passes safety check"""
        is_safe, error = check_input_safety("I want a medium risk portfolio for 5 years")
        assert is_safe
        assert error is None
    
    def test_disclaimer_enforcement(self):
        """Test: Disclaimer is added to output"""
        sample_output = {
            'portfolio_weights': {'RELIANCE.NS': 0.5, 'TCS.NS': 0.5},
            'metrics': {'expected_annual_return': 0.15},
            'explanation': 'This is a balanced portfolio.'
        }
        
        result = apply_output_guardrails(sample_output)
        assert 'disclaimer' in result['explanation'].lower()
    
    def test_output_validation(self):
        """Test: Invalid outputs are caught"""
        invalid_output = {
            'portfolio_weights': {'STOCK': 2.5},  # Invalid weight
            'metrics': {}
        }
        
        guardrail = GuardrailSystem()
        validation = guardrail.validate_portfolio_output(invalid_output)
        # Should have validation warnings
        assert len(validation.get('violations', [])) > 0


class TestAgenticMetrics:
    """Test suite for agentic evaluation metrics"""
    
    def test_tool_selection_accuracy(self):
        """Test: Tool selection metric works correctly"""
        metric = AgenticEvaluationMetrics()
        
        assert metric.tool_selection_accuracy('portfolio_optimizer', 'portfolio_optimizer') == 1.0
        assert metric.tool_selection_accuracy('portfolio_optimizer', 'wrong_tool') == 0.0
    
    def test_argument_correctness(self):
        """Test: Argument correctness scoring"""
        metric = AgenticEvaluationMetrics()
        
        expected = {'risk_profile': 'medium', 'horizon_years': 5}
        actual = {'risk_profile': 'medium', 'horizon_years': 5}
        
        assert metric.argument_correctness(expected, actual) == 1.0
        
        # Test partial match
        actual_partial = {'risk_profile': 'medium', 'horizon_years': 7}
        score = metric.argument_correctness(expected, actual_partial)
        assert 0.0 < score < 1.0
    
    def test_output_format_validity(self):
        """Test: Output format validation"""
        metric = AgenticEvaluationMetrics()
        
        valid_output = {
            'portfolio_weights': {'RELIANCE.NS': 0.6, 'TCS.NS': 0.4},
            'metrics': {'expected_annual_return': 0.15},
            'explanation': 'Portfolio explanation'
        }
        
        score = metric.output_format_validity(valid_output)
        assert score > 0.5
    
    def test_explanation_quality(self):
        """Test: Explanation quality scoring"""
        metric = AgenticEvaluationMetrics()
        
        good_explanation = """
        This portfolio is optimized for medium risk with a 5-year horizon.
        Expected annual return is 15% with 20% volatility.
        Diversification across technology and energy sectors.
        Disclaimer: This is not financial advice.
        """
        
        score = metric.explanation_quality(good_explanation)
        assert score > 0.5
        
        poor_explanation = "Here's your portfolio."
        score_poor = metric.explanation_quality(poor_explanation)
        assert score_poor < 0.3


# ========== Integration Test ==========

def test_end_to_end_pipeline():
    """
    End-to-end test: User query â†’ Portfolio â†’ Guardrails â†’ Response
    """
    # Simulate full pipeline (without agent for deterministic testing)
    user_query = "Medium risk, 7-year investment horizon"
    
    # Step 1: Input safety
    is_safe, error = check_input_safety(user_query)
    assert is_safe
    
    # Step 2: Portfolio optimization
    portfolio_result = portfolio_optimizer_tool(
        risk_profile='medium',
        horizon_years=7
    )
    assert 'portfolio_weights' in portfolio_result
    
    # Step 3: Output guardrails
    final_response = apply_output_guardrails(portfolio_result)
    assert final_response['success']
    assert 'disclaimer' in final_response['explanation'].lower()
    
    # Step 4: Evaluate with agentic metrics
    metrics = AgenticEvaluationMetrics()
    
    format_score = metrics.output_format_validity(final_response)
    explanation_score = metrics.explanation_quality(final_response['explanation'])
    guardrail_score = metrics.guardrail_compliance(final_response)
    
    print(f"\nðŸ“Š Agentic Evaluation Scores:")
    print(f"  â€¢ Output Format: {format_score:.2f}")
    print(f"  â€¢ Explanation Quality: {explanation_score:.2f}")
    print(f"  â€¢ Guardrail Compliance: {guardrail_score:.2f}")
    
    assert format_score > 0.5
    assert explanation_score > 0.3
    assert guardrail_score > 0.5


# ========== Evaluation Report Generator ==========

def generate_evaluation_report() -> Dict:
    """
    Generate comprehensive evaluation report for hackathon judges
    
    Returns:
        Evaluation metrics and test results
    """
    report = {
        "evaluation_framework": "Agentic AI Metrics (2025)",
        "test_results": {},
        "metric_scores": {},
        "compliance_checks": {}
    }
    
    # Run key tests
    try:
        test_end_to_end_pipeline()
        report["test_results"]["end_to_end"] = "PASS"
    except Exception as e:
        report["test_results"]["end_to_end"] = f"FAIL: {str(e)}"
    
    # Test guardrails
    guardrail_tests = [
        ("PII Detection", lambda: check_input_safety("PAN: ABCDE1234F")[0] == False),
        ("Safe Input", lambda: check_input_safety("Medium risk for 5 years")[0] == True)
    ]
    
    for test_name, test_func in guardrail_tests:
        try:
            result = test_func()
            report["compliance_checks"][test_name] = "PASS" if result else "FAIL"
        except:
            report["compliance_checks"][test_name] = "ERROR"
    
    return report


if __name__ == "__main__":
    print("="*70)
    print("F2 Portfolio Recommender Agent - Evaluation Framework")
    print("="*70)
    
    # Run pytest
    print("\nðŸ§ª Running Unit Tests...\n")
    pytest.main([__file__, "-v", "--tb=short"])
    
    # Generate evaluation report
    print("\n" + "="*70)
    print("ðŸ“Š Generating Evaluation Report...")
    print("="*70)
    
    report = generate_evaluation_report()
    
    print("\n" + json.dumps(report, indent=2))
    
    print("\nâœ… Evaluation Complete!")
    print("\nKey Metrics Demonstrated:")
    print("  â€¢ Tool Selection Accuracy")
    print("  â€¢ Argument Correctness")
    print("  â€¢ Output Format Validity")
    print("  â€¢ Explanation Quality")
    print("  â€¢ Guardrail Compliance")
    print("  â€¢ Reasoning Coherence")
